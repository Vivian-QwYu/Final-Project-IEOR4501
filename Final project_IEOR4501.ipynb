{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49b47bf",
   "metadata": {},
   "source": [
    "## Part 0: Setup\n",
    "In this part we set up some basic environment and variables needed for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe6aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import math\n",
    "import requests\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keplergl as kg\n",
    "from scipy import stats\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed8c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables needed\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV = [\"weather-2009.csv\", \"weather-2010.csv\", \"weather-2011.csv\", \n",
    "               \"weather-2012.csv\", \"weather-2013.csv\", \"weather-2014.csv\", \"weather-2015.csv\"]\n",
    "ZONE_PATH = \"taxi_zones.shp\"\n",
    "\n",
    "NY_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE = \"sqlite:///project.db\"\n",
    "SCHEMA_FILE = \"schema.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1366081",
   "metadata": {},
   "outputs": [],
   "source": [
    "Taxi_zone = gpd.read_file(ZONE_PATH)\n",
    "Taxi_zone = Taxi_zone.to_crs(4326)\n",
    "Taxi_zone['longitude'] = Taxi_zone.centroid.x  \n",
    "Taxi_zone['latitude'] = Taxi_zone.centroid.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d022e5",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "### Yellow Taxi trip data: Downloading, Cleaning, Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b889eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_html() -> bytes:\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019bea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_links() -> list:\n",
    "    links = []\n",
    "    pattern = r\"yellow_tripdata_2009|yellow_tripdata_201[0-4]|yellow_tripdata_2015-0[1-6]\"\n",
    "    soup = bs4.BeautifulSoup(get_taxi_html(),'html.parser')\n",
    "    for a in soup.find_all(\"a\",href = True):\n",
    "        link_text = a.get(\"href\")\n",
    "        matches = re.findall(pattern,link_text)\n",
    "        if matches:\n",
    "            links.append(link_text)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef193e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_taxi_data_download_clean_sample(url: str) -> pd.core.frame.DataFrame:\n",
    "    parquet_name = url.split(\"/\")[-1]\n",
    "\n",
    "    # download if it doesn't exist\n",
    "    if not os.path.exists(parquet_name):\n",
    "        print(f\"Downloading parquet for {parquet_name[16:23]}.\")\n",
    "        file = requests.get(url)\n",
    "        with open(parquet_name , \"wb\") as f:\n",
    "            f.write(file.content)\n",
    "    \n",
    "    # load data from parquet file\n",
    "    data = pd.read_parquet(parquet_name)\n",
    "    print(f\"Cleaning data for {parquet_name[16:23]}.\")\n",
    "    \n",
    "    # 为了不占用太多内存，读一个删一个，最后提交之前要删掉\n",
    "    os.remove(parquet_name) \n",
    "    print(f\"Parquet for {parquet_name[16:23]} is removed.\")\n",
    "    \n",
    "    # looking up the latitude and longitude for some months where only location IDs are given for pickups and dropoffs\n",
    "    # keep NaNs if exists\n",
    "    if \"PULocationID\" in data.columns:\n",
    "        data[\"pickup_latitude\"] = data[\"PULocationID\"].map(Taxi_zone[\"latitude\"], na_action = \"ignore\")\n",
    "        data[\"pickup_longitude\"] = data[\"PULocationID\"].map(Taxi_zone[\"longitude\"], na_action = \"ignore\")\n",
    "        data[\"dropoff_latitude\"] = data[\"DOLocationID\"].map(Taxi_zone[\"latitude\"], na_action = \"ignore\")\n",
    "        data[\"dropoff_longitude\"] = data[\"DOLocationID\"].map(Taxi_zone[\"longitude\"], na_action = \"ignore\")\n",
    "    \n",
    "    # normalize column names\n",
    "    rename_dict = {\n",
    "        \"VendorID\" : \"vendor_id\",\n",
    "        \"tpep_pickup_datetime\" : \"pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\" : \"dropoff_datetime\",\n",
    "        \"RatecodeID\" : \"rate_code\",\n",
    "        \"Trip_Pickup_DateTime\" : \"pickup_datetime\",\n",
    "        \"Trip_Dropoff_DateTime\" : \"dropoff_datetime\",\n",
    "        \"Start_Lon\" : \"pickup_longitude\",\n",
    "        \"Start_Lat\" : \"pickup_latitude\",\n",
    "        \"End_Lon\" : \"dropoff_longitude\",\n",
    "        \"End_Lat\" : \"dropoff_latitude\",\n",
    "        \"Fare_Amt\" : \"fare_amount\",\n",
    "        \"Tip_Amt\" : \"tip_amount\",\n",
    "        \"Tolls_Amt\" : \"tolls_amount\",\n",
    "        \"Total_Amt\" : \"total_amount\"\n",
    "    }\n",
    "    data.rename(columns = rename_dict, inplace = True)\n",
    "    \n",
    "    # remove the trips that the location IDs are be valid\n",
    "    data.dropna(subset=[\"pickup_latitude\",\"pickup_longitude\",\"dropoff_latitude\",\"dropoff_longitude\"],inplace = True)\n",
    "    \n",
    "    # remove invalid data points\n",
    "    data = data[data[\"total_amount\"] > 0]\n",
    "    \n",
    "    # normalize and use appropriate column types for the respective data\n",
    "    data[\"pickup_datetime\"] = pd.to_datetime(data[\"pickup_datetime\"])\n",
    "    data[\"dropoff_datetime\"] = pd.to_datetime(data[\"dropoff_datetime\"])\n",
    "    data = data.astype({\"pickup_latitude\": \"float64\",\"pickup_longitude\": \"float64\",\\\n",
    "                        \"dropoff_latitude\": \"float64\",\"dropoff_longitude\": \"float64\",\"tip_amount\": \"float64\"})\n",
    "    \n",
    "    # remove unnecessary columns and only keeping columns needed\n",
    "    data = data[[\"pickup_datetime\",\"pickup_latitude\",\"pickup_longitude\",\"dropoff_latitude\",\"dropoff_longitude\",\"tip_amount\"]]\n",
    "    \n",
    "    # remove trips that start and/or end outside of NY\n",
    "    data = data[(data[\"pickup_latitude\"] >= NY_COORDS[0][0]) & (data[\"pickup_latitude\"] <= NY_COORDS[1][0])]\n",
    "    data = data[(data[\"pickup_longitude\"] >= NY_COORDS[0][1]) & (data[\"pickup_longitude\"] <= NY_COORDS[1][1])]\n",
    "    data = data[(data[\"dropoff_latitude\"] >= NY_COORDS[0][0]) & (data[\"dropoff_latitude\"] <= NY_COORDS[1][0])]\n",
    "    data = data[(data[\"dropoff_longitude\"] >= NY_COORDS[0][1]) & (data[\"dropoff_longitude\"] <= NY_COORDS[1][1])]\n",
    "    \n",
    "    # Sampling\n",
    "    # Uber dataset consists of 200000 data points\n",
    "    # Therefore, we need 200000/78 ~ 2564 data points from each month\n",
    "    data = data.sample(2564)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcc595",
   "metadata": {},
   "source": [
    "### Yellow Taxi trip data: Filling (Distance)\n",
    "\n",
    "We calculate the distance between pickup location and dropoff location using the Haversine Formula:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/2789198/27240436-e9a459da-52d4-11e7-8f84-f96d0b312859.png)\n",
    "\n",
    "where $\\lambda$ and $\\phi$ are the `longitude` and `latitude` of locations respectively, $r$ is the radius of earth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd6abe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(pu_coord: pd.core.frame.DataFrame, do_coord: pd.core.frame.DataFrame) -> pd.core.series.Series:\n",
    "    \n",
    "    pick_lon = pu_coord[\"pickup_longitude\"].map(math.radians)\n",
    "    pick_lat = pu_coord[\"pickup_latitude\"].map(math.radians)\n",
    "    drop_lon = do_coord[\"dropoff_longitude\"].map(math.radians)\n",
    "    drop_lat = do_coord[\"dropoff_latitude\"].map(math.radians)\n",
    "    \n",
    "    delta_lat = drop_lat - pick_lat\n",
    "    delta_lon = drop_lon - pick_lon\n",
    "    \n",
    "    # Take the average earth radius (km) as r\n",
    "    r = 6371\n",
    "    part_formula = ((delta_lat/2).map(math.sin))**2 + (pick_lat.map(math.cos))*(drop_lat.map(math.cos))*((delta_lon/2).map(math.sin))**2\n",
    "    dist = 2 * r * part_formula.map(math.sqrt).map(math.asin)\n",
    "    \n",
    "    return dist.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547f8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filling_distance(data: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    pu_coord = data[[\"pickup_longitude\",\"pickup_latitude\"]]\n",
    "    do_coord = data[[\"dropoff_longitude\",\"dropoff_latitude\"]]\n",
    "    data[\"distance\"] = calculate_distance(pu_coord, do_coord)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66491c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_taxi_data(urls: list) -> pd.core.frame.DataFrame:\n",
    "    all_taxi_df = []\n",
    "    for url in urls:\n",
    "        data = monthly_taxi_data_download_clean_sample(url)\n",
    "        data = filling_distance(data)\n",
    "        all_taxi_df.append(data)\n",
    "    \n",
    "    all_data = pd.concat(all_taxi_df)\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaccf37c",
   "metadata": {},
   "source": [
    "### Uber rides data: Reading, Cleaning and Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12569d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uber_data_read_clean_fill() -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    data = pd.read_csv(UBER_CSV, low_memory = False)\n",
    "    print(\"Cleaning data for Uber rides.\")\n",
    "    \n",
    "    # remove the trips that the location IDs are be valid\n",
    "    data.dropna(subset=[\"pickup_latitude\",\"pickup_longitude\",\"dropoff_latitude\",\"dropoff_longitude\"],inplace = True)\n",
    "    \n",
    "    # normalize and use appropriate column types for the respective data\n",
    "    data[\"pickup_datetime\"] = pd.to_datetime(data[\"pickup_datetime\"])\n",
    "    data = data.astype({\"pickup_latitude\": \"float64\",\"pickup_longitude\": \"float64\",\\\n",
    "                        \"dropoff_latitude\": \"float64\",\"dropoff_longitude\": \"float64\"})\n",
    "    \n",
    "    # remove invalid data points\n",
    "    data = data[data[\"fare_amount\"] > 0]\n",
    "    \n",
    "    # remove unnecessary columns and only keeping columns needed \n",
    "    data = data[[\"pickup_datetime\",\"pickup_latitude\",\"pickup_longitude\",\"dropoff_latitude\",\"dropoff_longitude\"]]\n",
    "    \n",
    "    # remove trips that start and/or end outside of NY\n",
    "    data = data[(data[\"pickup_latitude\"] >= NY_COORDS[0][0]) & (data[\"pickup_latitude\"] <= NY_COORDS[1][0])]\n",
    "    data = data[(data[\"pickup_longitude\"] >= NY_COORDS[0][1]) & (data[\"pickup_longitude\"] <= NY_COORDS[1][1])]\n",
    "    data = data[(data[\"dropoff_latitude\"] >= NY_COORDS[0][0]) & (data[\"dropoff_latitude\"] <= NY_COORDS[1][0])]\n",
    "    data = data[(data[\"dropoff_longitude\"] >= NY_COORDS[0][1]) & (data[\"dropoff_longitude\"] <= NY_COORDS[1][1])]\n",
    "    \n",
    "    # fill in distance column\n",
    "    data = filling_distance(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f7396",
   "metadata": {},
   "source": [
    "### Weather data: Reading, Cleaning and Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "965f99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_weather_data_read_clean_fill(csv: str) -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    data = pd.read_csv(csv, low_memory = False)\n",
    "    \n",
    "    # remove columns of no use\n",
    "    data = data[[\"DATE\",\"HourlyWindSpeed\",\"HourlyPrecipitation\"]]\n",
    "\n",
    "    # remove missing values for wind speed data\n",
    "    data.dropna(subset=[\"HourlyWindSpeed\"], inplace=True)\n",
    "\n",
    "    # normalize and use appropriate column types for the respective data\n",
    "    data[\"DATE\"] = pd.to_datetime(data[\"DATE\"])\n",
    "    data[\"HourlyPrecipitation\"] = pd.to_numeric(data[\"HourlyPrecipitation\"], errors = \"coerce\")\n",
    "\n",
    "    # fill 0 to NAs in the precipitation data\n",
    "    data[\"HourlyPrecipitation\"].fillna(0, inplace=True)\n",
    "    data = data.astype({\"HourlyWindSpeed\":\"float64\", \"HourlyPrecipitation\":\"float64\"})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3629e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_weather_data_read_clean_fill(csv: str) -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    data = pd.read_csv(csv, low_memory = False)\n",
    "    data[\"DATE\"] = pd.to_datetime(data[\"DATE\"])\n",
    "    data[\"HourlyPrecipitation\"] = pd.to_numeric(data[\"HourlyPrecipitation\"], errors = \"coerce\")\n",
    "    data[\"HourlyPrecipitation\"].fillna(0, inplace = True)\n",
    "    \n",
    "    # Only take date into consideration now\n",
    "    data[\"DATE\"] = data[\"DATE\"].dt.date\n",
    "    data_daily = data.groupby([\"DATE\"],as_index = False).agg({\"HourlyPrecipitation\":\"sum\",\"HourlyWindSpeed\":\"mean\"})\n",
    "    data_daily.rename(columns = {\"HourlyPrecipitation\" : \"Precipitation\", \"HourlyWindSpeed\" : \"WindSpeed\"}, inplace = True)\n",
    "    \n",
    "    \n",
    "    data_daily[\"WindSpeed\"].round(2)\n",
    "    data_daily[\"DATE\"] = pd.to_datetime(data_daily[\"DATE\"])\n",
    "    data_daily = data_daily.astype({\"WindSpeed\":\"float64\", \"Precipitation\":\"float64\"})\n",
    "    \n",
    "    return data_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cad9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_sun_data_read(csv: str) -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    data = pd.read_csv(csv, low_memory = False)\n",
    "    data[\"DATE\"] = pd.to_datetime(data[\"DATE\"]).dt.date\n",
    "    data_sun = data.groupby([\"DATE\"], as_index = False).agg({\"Sunrise\":\"first\",\"Sunset\":\"first\"})\n",
    "    data_sun = data_sun.dropna()\n",
    "    data_sun[\"DATE\"] = pd.to_datetime(data_sun[\"DATE\"])\n",
    "    \n",
    "    # use appropriate column types for the respective data\n",
    "    data_sun = data_sun.astype({\"Sunrise\":\"int32\", \"Sunset\":\"int32\"})\n",
    "    data_sun = data_sun.astype({\"Sunrise\":\"string\", \"Sunset\":\"string\"})\n",
    "    \n",
    "    return data_sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f6e119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_weather_data() -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    hourly_data = []\n",
    "    daily_data = []\n",
    "    sun_data = []\n",
    "    \n",
    "    for csv in WEATHER_CSV:\n",
    "        ho_data = hourly_weather_data_read_clean_fill(csv)\n",
    "        da_data = daily_weather_data_read_clean_fill(csv)\n",
    "        su_data = daily_sun_data_read(csv)\n",
    "        \n",
    "        # for year 2015, only need data for the first six month\n",
    "        if csv == \"weather-2015.csv\":\n",
    "            ho_data = ho_data[ho_data[\"DATE\"].dt.month <= 6]\n",
    "            da_data = da_data[da_data[\"DATE\"].dt.month <= 6]\n",
    "            su_data = su_data[su_data[\"DATE\"].dt.month <= 6]\n",
    "            \n",
    "        hourly_data.append(ho_data)\n",
    "        daily_data.append(da_data)\n",
    "        sun_data.append(su_data)\n",
    "    \n",
    "    hour_data = pd.concat(hourly_data)\n",
    "    day_data = pd.concat(daily_data)\n",
    "    day_sun_data = pd.concat(sun_data)\n",
    "    \n",
    "    return hour_data, day_data, day_sun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc49e7",
   "metadata": {},
   "source": [
    "### Process all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e6df801",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data, daily_sun_data = all_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e5f6a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data for Uber rides.\n"
     ]
    }
   ],
   "source": [
    "uber_data = uber_data_read_clean_fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b86ebd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading parquet for 2015-01.\n",
      "Cleaning data for 2015-01.\n",
      "Parquet for 2015-01 is removed.\n",
      "Downloading parquet for 2015-02.\n",
      "Cleaning data for 2015-02.\n",
      "Parquet for 2015-02 is removed.\n",
      "Downloading parquet for 2015-03.\n",
      "Cleaning data for 2015-03.\n",
      "Parquet for 2015-03 is removed.\n",
      "Downloading parquet for 2015-04.\n",
      "Cleaning data for 2015-04.\n",
      "Parquet for 2015-04 is removed.\n",
      "Downloading parquet for 2015-05.\n",
      "Cleaning data for 2015-05.\n",
      "Parquet for 2015-05 is removed.\n",
      "Downloading parquet for 2015-06.\n",
      "Cleaning data for 2015-06.\n",
      "Parquet for 2015-06 is removed.\n",
      "Downloading parquet for 2014-01.\n",
      "Cleaning data for 2014-01.\n",
      "Parquet for 2014-01 is removed.\n",
      "Downloading parquet for 2014-02.\n",
      "Cleaning data for 2014-02.\n",
      "Parquet for 2014-02 is removed.\n",
      "Downloading parquet for 2014-03.\n",
      "Cleaning data for 2014-03.\n",
      "Parquet for 2014-03 is removed.\n",
      "Downloading parquet for 2014-04.\n",
      "Cleaning data for 2014-04.\n",
      "Parquet for 2014-04 is removed.\n",
      "Downloading parquet for 2014-05.\n",
      "Cleaning data for 2014-05.\n",
      "Parquet for 2014-05 is removed.\n",
      "Downloading parquet for 2014-06.\n",
      "Cleaning data for 2014-06.\n",
      "Parquet for 2014-06 is removed.\n",
      "Downloading parquet for 2014-07.\n",
      "Cleaning data for 2014-07.\n",
      "Parquet for 2014-07 is removed.\n",
      "Downloading parquet for 2014-08.\n",
      "Cleaning data for 2014-08.\n",
      "Parquet for 2014-08 is removed.\n",
      "Downloading parquet for 2014-09.\n",
      "Cleaning data for 2014-09.\n",
      "Parquet for 2014-09 is removed.\n",
      "Downloading parquet for 2014-10.\n",
      "Cleaning data for 2014-10.\n",
      "Parquet for 2014-10 is removed.\n",
      "Downloading parquet for 2014-11.\n",
      "Cleaning data for 2014-11.\n",
      "Parquet for 2014-11 is removed.\n",
      "Downloading parquet for 2014-12.\n",
      "Cleaning data for 2014-12.\n",
      "Parquet for 2014-12 is removed.\n",
      "Downloading parquet for 2013-01.\n",
      "Cleaning data for 2013-01.\n",
      "Parquet for 2013-01 is removed.\n",
      "Downloading parquet for 2013-02.\n",
      "Cleaning data for 2013-02.\n",
      "Parquet for 2013-02 is removed.\n",
      "Downloading parquet for 2013-03.\n",
      "Cleaning data for 2013-03.\n",
      "Parquet for 2013-03 is removed.\n",
      "Downloading parquet for 2013-04.\n",
      "Cleaning data for 2013-04.\n",
      "Parquet for 2013-04 is removed.\n",
      "Downloading parquet for 2013-05.\n",
      "Cleaning data for 2013-05.\n",
      "Parquet for 2013-05 is removed.\n",
      "Downloading parquet for 2013-06.\n",
      "Cleaning data for 2013-06.\n",
      "Parquet for 2013-06 is removed.\n",
      "Downloading parquet for 2013-07.\n",
      "Cleaning data for 2013-07.\n",
      "Parquet for 2013-07 is removed.\n",
      "Downloading parquet for 2013-08.\n",
      "Cleaning data for 2013-08.\n",
      "Parquet for 2013-08 is removed.\n",
      "Downloading parquet for 2013-09.\n",
      "Cleaning data for 2013-09.\n",
      "Parquet for 2013-09 is removed.\n",
      "Downloading parquet for 2013-10.\n",
      "Cleaning data for 2013-10.\n",
      "Parquet for 2013-10 is removed.\n",
      "Downloading parquet for 2013-11.\n",
      "Cleaning data for 2013-11.\n",
      "Parquet for 2013-11 is removed.\n",
      "Downloading parquet for 2013-12.\n",
      "Cleaning data for 2013-12.\n",
      "Parquet for 2013-12 is removed.\n",
      "Downloading parquet for 2012-01.\n",
      "Cleaning data for 2012-01.\n",
      "Parquet for 2012-01 is removed.\n",
      "Downloading parquet for 2012-02.\n",
      "Cleaning data for 2012-02.\n",
      "Parquet for 2012-02 is removed.\n",
      "Downloading parquet for 2012-03.\n",
      "Cleaning data for 2012-03.\n",
      "Parquet for 2012-03 is removed.\n",
      "Downloading parquet for 2012-04.\n",
      "Cleaning data for 2012-04.\n",
      "Parquet for 2012-04 is removed.\n",
      "Downloading parquet for 2012-05.\n",
      "Cleaning data for 2012-05.\n",
      "Parquet for 2012-05 is removed.\n",
      "Downloading parquet for 2012-06.\n",
      "Cleaning data for 2012-06.\n",
      "Parquet for 2012-06 is removed.\n",
      "Downloading parquet for 2012-07.\n",
      "Cleaning data for 2012-07.\n",
      "Parquet for 2012-07 is removed.\n",
      "Downloading parquet for 2012-08.\n",
      "Cleaning data for 2012-08.\n",
      "Parquet for 2012-08 is removed.\n",
      "Downloading parquet for 2012-09.\n",
      "Cleaning data for 2012-09.\n",
      "Parquet for 2012-09 is removed.\n",
      "Downloading parquet for 2012-10.\n",
      "Cleaning data for 2012-10.\n",
      "Parquet for 2012-10 is removed.\n",
      "Downloading parquet for 2012-11.\n",
      "Cleaning data for 2012-11.\n",
      "Parquet for 2012-11 is removed.\n",
      "Downloading parquet for 2012-12.\n",
      "Cleaning data for 2012-12.\n",
      "Parquet for 2012-12 is removed.\n",
      "Downloading parquet for 2011-01.\n",
      "Cleaning data for 2011-01.\n",
      "Parquet for 2011-01 is removed.\n",
      "Downloading parquet for 2011-02.\n",
      "Cleaning data for 2011-02.\n",
      "Parquet for 2011-02 is removed.\n",
      "Downloading parquet for 2011-03.\n",
      "Cleaning data for 2011-03.\n",
      "Parquet for 2011-03 is removed.\n",
      "Downloading parquet for 2011-04.\n",
      "Cleaning data for 2011-04.\n",
      "Parquet for 2011-04 is removed.\n",
      "Downloading parquet for 2011-05.\n",
      "Cleaning data for 2011-05.\n",
      "Parquet for 2011-05 is removed.\n",
      "Downloading parquet for 2011-06.\n",
      "Cleaning data for 2011-06.\n",
      "Parquet for 2011-06 is removed.\n",
      "Downloading parquet for 2011-07.\n",
      "Cleaning data for 2011-07.\n",
      "Parquet for 2011-07 is removed.\n",
      "Downloading parquet for 2011-08.\n",
      "Cleaning data for 2011-08.\n",
      "Parquet for 2011-08 is removed.\n",
      "Downloading parquet for 2011-09.\n",
      "Cleaning data for 2011-09.\n",
      "Parquet for 2011-09 is removed.\n",
      "Downloading parquet for 2011-10.\n",
      "Cleaning data for 2011-10.\n",
      "Parquet for 2011-10 is removed.\n",
      "Downloading parquet for 2011-11.\n",
      "Cleaning data for 2011-11.\n",
      "Parquet for 2011-11 is removed.\n",
      "Downloading parquet for 2011-12.\n",
      "Cleaning data for 2011-12.\n",
      "Parquet for 2011-12 is removed.\n",
      "Downloading parquet for 2010-01.\n",
      "Cleaning data for 2010-01.\n",
      "Parquet for 2010-01 is removed.\n",
      "Downloading parquet for 2010-02.\n",
      "Cleaning data for 2010-02.\n",
      "Parquet for 2010-02 is removed.\n",
      "Downloading parquet for 2010-03.\n",
      "Cleaning data for 2010-03.\n",
      "Parquet for 2010-03 is removed.\n",
      "Downloading parquet for 2010-04.\n",
      "Cleaning data for 2010-04.\n",
      "Parquet for 2010-04 is removed.\n",
      "Downloading parquet for 2010-05.\n",
      "Cleaning data for 2010-05.\n",
      "Parquet for 2010-05 is removed.\n",
      "Downloading parquet for 2010-06.\n",
      "Cleaning data for 2010-06.\n",
      "Parquet for 2010-06 is removed.\n",
      "Downloading parquet for 2010-07.\n",
      "Cleaning data for 2010-07.\n",
      "Parquet for 2010-07 is removed.\n",
      "Downloading parquet for 2010-08.\n",
      "Cleaning data for 2010-08.\n",
      "Parquet for 2010-08 is removed.\n",
      "Downloading parquet for 2010-09.\n",
      "Cleaning data for 2010-09.\n",
      "Parquet for 2010-09 is removed.\n",
      "Downloading parquet for 2010-10.\n",
      "Cleaning data for 2010-10.\n",
      "Parquet for 2010-10 is removed.\n",
      "Downloading parquet for 2010-11.\n",
      "Cleaning data for 2010-11.\n",
      "Parquet for 2010-11 is removed.\n",
      "Downloading parquet for 2010-12.\n",
      "Cleaning data for 2010-12.\n",
      "Parquet for 2010-12 is removed.\n",
      "Downloading parquet for 2009-01.\n",
      "Cleaning data for 2009-01.\n",
      "Parquet for 2009-01 is removed.\n",
      "Downloading parquet for 2009-02.\n",
      "Cleaning data for 2009-02.\n",
      "Parquet for 2009-02 is removed.\n",
      "Downloading parquet for 2009-03.\n",
      "Cleaning data for 2009-03.\n",
      "Parquet for 2009-03 is removed.\n",
      "Downloading parquet for 2009-04.\n",
      "Cleaning data for 2009-04.\n",
      "Parquet for 2009-04 is removed.\n",
      "Downloading parquet for 2009-05.\n",
      "Cleaning data for 2009-05.\n",
      "Parquet for 2009-05 is removed.\n",
      "Downloading parquet for 2009-06.\n",
      "Cleaning data for 2009-06.\n",
      "Parquet for 2009-06 is removed.\n",
      "Downloading parquet for 2009-07.\n",
      "Cleaning data for 2009-07.\n",
      "Parquet for 2009-07 is removed.\n",
      "Downloading parquet for 2009-08.\n",
      "Cleaning data for 2009-08.\n",
      "Parquet for 2009-08 is removed.\n",
      "Downloading parquet for 2009-09.\n",
      "Cleaning data for 2009-09.\n",
      "Parquet for 2009-09 is removed.\n",
      "Downloading parquet for 2009-10.\n",
      "Cleaning data for 2009-10.\n",
      "Parquet for 2009-10 is removed.\n",
      "Downloading parquet for 2009-11.\n",
      "Cleaning data for 2009-11.\n",
      "Parquet for 2009-11 is removed.\n",
      "Downloading parquet for 2009-12.\n",
      "Cleaning data for 2009-12.\n",
      "Parquet for 2009-12 is removed.\n"
     ]
    }
   ],
   "source": [
    "links = find_taxi_parquet_links()\n",
    "taxi_data = all_taxi_data(links) # 加了个范围"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293dd77",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data\n",
    "### Create database and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "825ea545",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1327c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define statements to create tables\n",
    "\n",
    "Taxi_STMT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trip\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_latitude FLOAT64,\n",
    "    pickup_longitude FLOAT64,\n",
    "    dropoff_latitude FLOAT64,\n",
    "    dropoff_longitude FLOAT64,\n",
    "    tip_amount FLOAT64,\n",
    "    distance FLOAT64\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "Uber_STMT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trip\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_latitude FLOAT64,\n",
    "    pickup_longitude FLOAT64,\n",
    "    dropoff_latitude FLOAT64,\n",
    "    dropoff_longitude FLOAT64,\n",
    "    distance FLOAT64\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "Hourly_Weather_STMT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    HourlyWindSpeed FLOAT64,\n",
    "    HourlyPrecipitation FLOAT64\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "Daily_Weather_STMT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    Precipitation FLOAT64,\n",
    "    WindSpeed FLOAT64\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "Sun_STMT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_sun\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    Sunrise STRING,\n",
    "    Sunset STRING\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17b2e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(Taxi_STMT)\n",
    "    connection.execute(Uber_STMT)\n",
    "    connection.execute(Hourly_Weather_STMT)\n",
    "    connection.execute(Daily_Weather_STMT)\n",
    "    connection.execute(Sun_STMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48143f15",
   "metadata": {},
   "source": [
    "### Add cleaned data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5f76a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi_data added.\n",
      "Uber_data added.\n",
      "Hourly_weather_data added.\n",
      "Daily_weather_data added.\n",
      "Sun_data added.\n"
     ]
    }
   ],
   "source": [
    "# add data from the dataframes to corresponding SQL tables\n",
    "taxi_data.to_sql(\"taxi_trip\", engine, if_exists = \"replace\", index = False)\n",
    "print(\"Taxi_data added.\")\n",
    "uber_data.to_sql(\"uber_trip\", engine, if_exists = \"replace\", index = False)\n",
    "print(\"Uber_data added.\")\n",
    "hourly_weather_data.to_sql(\"hourly_weather\", engine, if_exists = \"replace\", index = False)\n",
    "print(\"Hourly_weather_data added.\")\n",
    "daily_weather_data.to_sql(\"daily_weather\", engine, if_exists = \"replace\", index = False)\n",
    "print(\"Daily_weather_data added.\")\n",
    "daily_sun_data.to_sql(\"daily_sun\", engine, if_exists = \"replace\", index = False)\n",
    "print(\"Sun_data added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1bbf2",
   "metadata": {},
   "source": [
    "### Create a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c331ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(Taxi_STMT)\n",
    "    f.write(Uber_STMT)\n",
    "    f.write(Hourly_Weather_STMT)\n",
    "    f.write(Daily_Weather_STMT)\n",
    "    f.write(Sun_STMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41e4df",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Data\n",
    "First of all, for simplicity, define a function to store the query files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b94edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_file(query: str, file_name: str) -> None:\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9db6d8",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "For 01-2009 through 06-2015, what hour of the day was the most popular to take a Yellow Taxi? The result should have 24 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3762b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query_file name for question 1\n",
    "query_file_1 = \"popular_hour_for_taxi.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65aa6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query\n",
    "QUERY_STMT_1 = \"\"\"\n",
    "SELECT strftime (\"%H\",pickup_datetime) AS hour, COUNT(*) AS counts\n",
    "FROM taxi_trip\n",
    "GROUP BY hour\n",
    "ORDER BY counts DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05f2d487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('19', 12546),\n",
       " ('18', 12068),\n",
       " ('20', 12005),\n",
       " ('21', 11610),\n",
       " ('22', 11050),\n",
       " ('14', 9997),\n",
       " ('17', 9922),\n",
       " ('12', 9884),\n",
       " ('23', 9871),\n",
       " ('13', 9733),\n",
       " ('15', 9572),\n",
       " ('09', 9356),\n",
       " ('11', 9212),\n",
       " ('08', 9114),\n",
       " ('10', 8951),\n",
       " ('16', 8218),\n",
       " ('00', 7971),\n",
       " ('07', 7116),\n",
       " ('01', 5814),\n",
       " ('02', 4457),\n",
       " ('06', 4083),\n",
       " ('03', 3181),\n",
       " ('04', 2326),\n",
       " ('05', 1935)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the query\n",
    "engine.execute(QUERY_STMT_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfa0ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query to file\n",
    "query_to_file(QUERY_STMT_1, query_file_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c4cf8",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "For the same time frame, what day of the week was the most popular to take an Uber? The result should have 7 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10d921b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query_file name for question 2\n",
    "query_file_2 = \"popular_day_for_uber.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e69647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query\n",
    "QUERY_STMT_2 = \"\"\"\n",
    "SELECT strftime (\"%w\",pickup_datetime) AS day, COUNT(*) AS counts\n",
    "FROM uber_trip\n",
    "GROUP BY day\n",
    "ORDER BY counts DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fe14deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5', 30161),\n",
       " ('6', 29598),\n",
       " ('4', 29335),\n",
       " ('3', 28326),\n",
       " ('2', 27520),\n",
       " ('0', 25833),\n",
       " ('1', 24681)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the query\n",
    "engine.execute(QUERY_STMT_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "755bb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query to file\n",
    "query_to_file(QUERY_STMT_2, query_file_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16dd495",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "What is the 95% percentile of distance traveled for all hired trips during July 2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94662129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query_file name for question 3\n",
    "query_file_3 = \"July_2013_95percentile.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d3f71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query\n",
    "QUERY_STMT_3 = \"\"\"\n",
    "WITH hired_trip AS (\n",
    "SELECT pickup_datetime, distance FROM taxi_trip\n",
    "WHERE pickup_datetime BETWEEN \"2013-07-01\" AND \"2013-08-01\"\n",
    "UNION ALL\n",
    "SELECT pickup_datetime, distance FROM uber_trip\n",
    "WHERE pickup_datetime BETWEEN \"2013-07-01\" AND \"2013-08-01\")\n",
    "\n",
    "SELECT distance AS \"95%_percentile_of_distance\" FROM hired_trip\n",
    "ORDER BY distance ASC\n",
    "LIMIT 1\n",
    "OFFSET (\n",
    "SELECT COUNT(*)\n",
    "FROM hired_trip\n",
    ") * 95/100 -1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1442abfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23.669217067439067,)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the query\n",
    "engine.execute(QUERY_STMT_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a17522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query to file\n",
    "query_to_file(QUERY_STMT_3, query_file_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61348bc",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52d03e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query_file name for question 4\n",
    "query_file_4 = \"2009_top_days_and_average.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b30f0d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query\n",
    "QUERY_STMT_4 = \"\"\"\n",
    "WITH hired_trip AS (\n",
    "SELECT pickup_datetime, distance FROM taxi_trip\n",
    "WHERE pickup_datetime BETWEEN \"2009-01-01\" AND \"2010-01-01\"\n",
    "UNION ALL\n",
    "SELECT pickup_datetime, distance FROM uber_trip\n",
    "WHERE pickup_datetime BETWEEN \"2009-01-01\" AND \"2010-01-01\")\n",
    "\n",
    "SELECT date(pickup_datetime) AS date, AVG(distance) AS avg_distance, COUNT(*) AS number\n",
    "FROM hired_trip\n",
    "GROUP BY date\n",
    "ORDER BY number DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b2627da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2009-10-23', 2.6649891026222026, 239),\n",
       " ('2009-01-31', 2.8255293266170147, 222),\n",
       " ('2009-09-16', 2.7365888436434522, 217),\n",
       " ('2009-06-05', 2.99244699791889, 216),\n",
       " ('2009-12-11', 3.106160260795538, 214),\n",
       " ('2009-12-18', 3.063084657688641, 212),\n",
       " ('2009-02-20', 3.2944825626359457, 211),\n",
       " ('2009-04-04', 2.918860697120937, 208),\n",
       " ('2009-03-19', 3.1379530533614446, 208),\n",
       " ('2009-11-13', 3.025966732349884, 206)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 需要修改query名字\n",
    "engine.execute(QUERY_STMT_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12d3a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query to file\n",
    "query_to_file(QUERY_STMT_4, query_file_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9439bcc",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Which 10 days in 2014 were the windiest on average, and how many hired trips were made on those days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "164d6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query_file name for question 5\n",
    "query_file_5 = \"2014_windiest_days_and_number.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54a535ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query\n",
    "QUERY_STMT_5 = \"\"\"\n",
    "WITH hired_trip AS (\n",
    "SELECT date(pickup_datetime) AS date_day FROM taxi_trip\n",
    "WHERE pickup_datetime BETWEEN \"2014-01-01\" AND \"2015-01-01\"\n",
    "UNION ALL\n",
    "SELECT date(pickup_datetime) AS date_day FROM uber_trip\n",
    "WHERE pickup_datetime BETWEEN \"2014-01-01\" AND \"2015-01-01\")\n",
    "\n",
    "SELECT date_day AS date, COUNT(*) AS number, WindSpeed\n",
    "FROM hired_trip LEFT JOIN(\n",
    "SELECT date(DATE) as day,WindSpeed FROM daily_weather \n",
    "WHERE DATE BETWEEN \"2014-01-01\" AND \"2015-01-01\"\n",
    "ORDER BY WindSpeed DESC \n",
    "LIMIT 10) ON date = day\n",
    "GROUP BY date\n",
    "ORDER BY WindSpeed DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab237ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2014-03-13', 191, 14.0),\n",
       " ('2014-01-07', 170, 13.083333333333334),\n",
       " ('2014-01-02', 131, 12.727272727272727),\n",
       " ('2014-02-13', 137, 12.226415094339623),\n",
       " ('2014-03-26', 176, 11.954545454545455),\n",
       " ('2014-03-29', 211, 11.914893617021276),\n",
       " ('2014-12-07', 164, 11.6),\n",
       " ('2014-12-09', 154, 11.26923076923077),\n",
       " ('2014-12-08', 161, 11.266666666666667),\n",
       " ('2014-11-02', 158, 10.826086956521738)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the query\n",
    "engine.execute(QUERY_STMT_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78526acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query to file\n",
    "query_to_file(QUERY_STMT_5, query_file_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668b279",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "During Hurricane Sandy in NYC (Oct 29-30, 2012), plus the week leading up and the week after, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed? There should be an entry for every single hour, even if no rides were taken, no precipitation was measured, or there was no wind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a43a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query_file name for question 6\n",
    "query_file_6 = \"Hurricane_Sandy_weather_trips.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96d94ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query\n",
    "QUERY_STMT_6 = \"\"\"\n",
    "WITH RECURSIVE hours(x) AS (\n",
    "SELECT '2012-10-22 00:00:00'AS hour\n",
    "UNION ALL\n",
    "SELECT DATETIME(x, '+1 HOUR') AS hour FROM hours WHERE x<'2012-11-06 23:59:59')\n",
    "\n",
    "SELECT strftime('%Y-%m-%d %H',x) AS time, COUNT(trip_time) AS number, AVG(HourlyWindSpeed) AS WindSpeed, SUM(HourlyPrecipitation) AS Precipitation\n",
    "FROM hours LEFT OUTER JOIN hourly_weather ON strftime('%Y-%m-%d %H',DATE) = time\n",
    "LEFT OUTER JOIN \n",
    "(\n",
    "SELECT strftime('%Y-%m-%d %H',pickup_datetime) AS trip_time\n",
    "FROM taxi_trip\n",
    "WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-11-07'\n",
    "UNION ALL\n",
    "SELECT strftime('%Y-%m-%d %H',pickup_datetime) AS trip_time\n",
    "FROM uber_trip\n",
    "WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-11-07')\n",
    "ON trip_time = time\n",
    "GROUP BY time;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2b3cc10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2012-10-22 00', 8, 7.0, 0.0),\n",
       " ('2012-10-22 01', 1, 5.0, 0.0),\n",
       " ('2012-10-22 02', 2, 7.0, 0.0),\n",
       " ('2012-10-22 03', 1, 0.0, 0.0),\n",
       " ('2012-10-22 04', 1, 0.0, 0.0),\n",
       " ('2012-10-22 05', 2, 0.0, 0.0),\n",
       " ('2012-10-22 06', 8, 5.0, 0.0),\n",
       " ('2012-10-22 07', 9, 3.0, 0.0),\n",
       " ('2012-10-22 08', 4, 3.0, 0.0),\n",
       " ('2012-10-22 09', 13, 5.0, 0.0),\n",
       " ('2012-10-22 10', 10, None, None),\n",
       " ('2012-10-22 11', 11, None, None),\n",
       " ('2012-10-22 12', 6, 11.0, 0.0),\n",
       " ('2012-10-22 13', 8, None, None),\n",
       " ('2012-10-22 14', 13, 7.0, 0.0),\n",
       " ('2012-10-22 15', 4, 6.0, 0.0),\n",
       " ('2012-10-22 16', 9, 3.0, 0.0),\n",
       " ('2012-10-22 17', 10, 7.0, 0.0),\n",
       " ('2012-10-22 18', 15, 5.0, 0.0),\n",
       " ('2012-10-22 19', 9, 5.0, 0.0),\n",
       " ('2012-10-22 20', 9, 3.0, 0.0),\n",
       " ('2012-10-22 21', 2, 0.0, 0.0),\n",
       " ('2012-10-22 22', 11, 3.0, 0.0),\n",
       " ('2012-10-22 23', 7, 3.0, 0.0),\n",
       " ('2012-10-23 00', 3, 3.0, 0.0),\n",
       " ('2012-10-23 01', 2, 0.0, 0.0),\n",
       " ('2012-10-23 02', 1, 3.0, 0.0),\n",
       " ('2012-10-23 03', 1, 0.0, 0.0),\n",
       " ('2012-10-23 04', 0, 3.0, 0.0),\n",
       " ('2012-10-23 05', 4, 0.0, 0.0),\n",
       " ('2012-10-23 06', 5, 0.0, 0.0),\n",
       " ('2012-10-23 07', 11, 0.0, 0.0),\n",
       " ('2012-10-23 08', 10, 0.0, 0.0),\n",
       " ('2012-10-23 09', 11, 3.0, 0.0),\n",
       " ('2012-10-23 10', 9, 0.0, 0.0),\n",
       " ('2012-10-23 11', 8, 3.0, 0.0),\n",
       " ('2012-10-23 12', 6, 0.0, 0.0),\n",
       " ('2012-10-23 13', 11, None, None),\n",
       " ('2012-10-23 14', 12, None, None),\n",
       " ('2012-10-23 15', 9, None, None),\n",
       " ('2012-10-23 16', 4, 3.0, 0.0),\n",
       " ('2012-10-23 17', 10, None, None),\n",
       " ('2012-10-23 18', 11, 5.0, 0.0),\n",
       " ('2012-10-23 19', 12, 0.0, 0.0),\n",
       " ('2012-10-23 20', 13, 0.0, 0.25999999999999995),\n",
       " ('2012-10-23 21', 12, 5.0, 0.0),\n",
       " ('2012-10-23 22', 12, 0.0, 0.11999999999999998),\n",
       " ('2012-10-23 23', 13, 5.0, 0.0),\n",
       " ('2012-10-24 00', 0, 3.0, 0.0),\n",
       " ('2012-10-24 01', 2, 5.5, 0.0),\n",
       " ('2012-10-24 02', 16, 5.75, 0.0),\n",
       " ('2012-10-24 03', 0, 7.0, 0.0),\n",
       " ('2012-10-24 04', 1, 7.0, 0.0),\n",
       " ('2012-10-24 05', 3, 6.0, 0.0),\n",
       " ('2012-10-24 06', 4, 4.0, 0.0),\n",
       " ('2012-10-24 07', 9, 1.6666666666666667, 0.0),\n",
       " ('2012-10-24 08', 8, 0.0, 0.0),\n",
       " ('2012-10-24 09', 10, 1.5, 0.0),\n",
       " ('2012-10-24 10', 4, 7.0, 0.0),\n",
       " ('2012-10-24 11', 6, 7.0, 0.0),\n",
       " ('2012-10-24 12', 24, 7.333333333333333, 0.0),\n",
       " ('2012-10-24 13', 9, 8.0, 0.0),\n",
       " ('2012-10-24 14', 8, 6.5, 0.0),\n",
       " ('2012-10-24 15', 6, 7.0, 0.0),\n",
       " ('2012-10-24 16', 6, 8.0, 0.0),\n",
       " ('2012-10-24 17', 6, 5.0, 0.0),\n",
       " ('2012-10-24 18', 11, 7.0, 0.0),\n",
       " ('2012-10-24 19', 10, 8.0, 0.0),\n",
       " ('2012-10-24 20', 30, 1.5, 0.0),\n",
       " ('2012-10-24 21', 30, 4.0, 0.0),\n",
       " ('2012-10-24 22', 14, 5.5, 0.0),\n",
       " ('2012-10-24 23', 7, 0.0, 0.0),\n",
       " ('2012-10-25 00', 18, 4.5, 0.0),\n",
       " ('2012-10-25 01', 4, 3.0, 0.0),\n",
       " ('2012-10-25 02', 16, 3.0, 0.0),\n",
       " ('2012-10-25 03', 0, 6.5, 0.0),\n",
       " ('2012-10-25 04', 1, 6.0, 0.0),\n",
       " ('2012-10-25 05', 5, 0.0, 0.0),\n",
       " ('2012-10-25 06', 7, 5.0, 0.0),\n",
       " ('2012-10-25 07', 6, 6.0, 0.0),\n",
       " ('2012-10-25 08', 12, 5.0, 0.0),\n",
       " ('2012-10-25 09', 8, 3.0, 0.0),\n",
       " ('2012-10-25 10', 5, 6.0, 0.0),\n",
       " ('2012-10-25 11', 8, 0.0, 0.0),\n",
       " ('2012-10-25 12', 11, 6.0, 0.0),\n",
       " ('2012-10-25 13', 8, 0.0, 0.0),\n",
       " ('2012-10-25 14', 6, 5.0, 0.0),\n",
       " ('2012-10-25 15', 8, 5.0, 0.0),\n",
       " ('2012-10-25 16', 4, 0.0, 0.0),\n",
       " ('2012-10-25 17', 12, 3.0, 0.0),\n",
       " ('2012-10-25 18', 14, 0.0, 0.0),\n",
       " ('2012-10-25 19', 5, 0.0, 0.0),\n",
       " ('2012-10-25 20', 25, 3.0, 0.0),\n",
       " ('2012-10-25 21', 13, 3.0, 0.0),\n",
       " ('2012-10-25 22', 9, 3.0, 0.0),\n",
       " ('2012-10-25 23', 11, 0.0, 0.0),\n",
       " ('2012-10-26 00', 15, 2.6666666666666665, 0.0),\n",
       " ('2012-10-26 01', 3, 0.0, 0.0),\n",
       " ('2012-10-26 02', 6, 0.0, 0.0),\n",
       " ('2012-10-26 03', 4, 3.0, 0.0),\n",
       " ('2012-10-26 04', 2, 0.0, 0.0),\n",
       " ('2012-10-26 05', 1, 0.0, 0.0),\n",
       " ('2012-10-26 06', 2, 0.0, 0.0),\n",
       " ('2012-10-26 07', 10, 3.0, 0.0),\n",
       " ('2012-10-26 08', 1, 3.0, 0.0),\n",
       " ('2012-10-26 09', 17, 3.0, 0.0),\n",
       " ('2012-10-26 10', 5, 3.0, 0.0),\n",
       " ('2012-10-26 11', 9, 3.0, 0.0),\n",
       " ('2012-10-26 12', 10, 0.0, 0.0),\n",
       " ('2012-10-26 13', 12, 3.0, 0.0),\n",
       " ('2012-10-26 14', 9, 3.0, 0.0),\n",
       " ('2012-10-26 15', 7, 0.0, 0.0),\n",
       " ('2012-10-26 16', 4, 0.0, 0.0),\n",
       " ('2012-10-26 17', 9, 0.0, 0.0),\n",
       " ('2012-10-26 18', 10, 0.0, 0.0),\n",
       " ('2012-10-26 19', 9, 0.0, 0.0),\n",
       " ('2012-10-26 20', 13, 3.0, 0.0),\n",
       " ('2012-10-26 21', 10, 3.0, 0.0),\n",
       " ('2012-10-26 22', 12, 0.0, 0.0),\n",
       " ('2012-10-26 23', 12, 0.0, 0.0),\n",
       " ('2012-10-27 00', 12, 3.0, 0.0),\n",
       " ('2012-10-27 01', 5, 0.0, 0.0),\n",
       " ('2012-10-27 02', 12, 3.0, 0.0),\n",
       " ('2012-10-27 03', 7, 0.0, 0.0),\n",
       " ('2012-10-27 04', 2, 6.0, 0.0),\n",
       " ('2012-10-27 05', 4, 6.0, 0.0),\n",
       " ('2012-10-27 06', 3, 6.0, 0.0),\n",
       " ('2012-10-27 07', 4, 4.0, 0.0),\n",
       " ('2012-10-27 08', 9, 6.0, 0.0),\n",
       " ('2012-10-27 09', 9, 6.0, 0.0),\n",
       " ('2012-10-27 10', 8, 8.0, 0.0),\n",
       " ('2012-10-27 11', 20, 5.75, 0.0),\n",
       " ('2012-10-27 12', 7, 8.0, 0.0),\n",
       " ('2012-10-27 13', 10, 8.0, 0.0),\n",
       " ('2012-10-27 14', 5, 10.0, 0.0),\n",
       " ('2012-10-27 15', 5, 10.0, 0.0),\n",
       " ('2012-10-27 16', 10, 7.0, 0.0),\n",
       " ('2012-10-27 17', 13, 7.0, 0.0),\n",
       " ('2012-10-27 18', 9, 7.0, 0.0),\n",
       " ('2012-10-27 19', 42, 8.0, 0.0),\n",
       " ('2012-10-27 20', 18, 7.0, 0.0),\n",
       " ('2012-10-27 21', 10, 9.0, 0.0),\n",
       " ('2012-10-27 22', 14, 9.0, 0.0),\n",
       " ('2012-10-27 23', 11, 8.0, 0.0),\n",
       " ('2012-10-28 00', 12, 11.0, 0.0),\n",
       " ('2012-10-28 01', 16, 8.0, 0.0),\n",
       " ('2012-10-28 02', 7, 8.0, 0.0),\n",
       " ('2012-10-28 03', 5, 9.0, 0.0),\n",
       " ('2012-10-28 04', 4, 10.0, 0.0),\n",
       " ('2012-10-28 05', 2, 11.0, 0.0),\n",
       " ('2012-10-28 06', 4, 10.0, 0.0),\n",
       " ('2012-10-28 07', 2, 11.0, 0.0),\n",
       " ('2012-10-28 08', 1, 11.0, 0.0),\n",
       " ('2012-10-28 09', 9, 11.0, 0.0),\n",
       " ('2012-10-28 10', 3, 10.0, 0.0),\n",
       " ('2012-10-28 11', 12, 8.0, 0.0),\n",
       " ('2012-10-28 12', 7, 7.0, 0.0),\n",
       " ('2012-10-28 13', 10, 13.0, 0.0),\n",
       " ('2012-10-28 14', 10, 13.0, 0.0),\n",
       " ('2012-10-28 15', 10, 13.0, 0.0),\n",
       " ('2012-10-28 16', 8, 16.0, 0.0),\n",
       " ('2012-10-28 17', 10, 11.0, 0.0),\n",
       " ('2012-10-28 18', 7, 15.0, 0.0),\n",
       " ('2012-10-28 19', 7, 14.0, 0.0),\n",
       " ('2012-10-28 20', 6, 16.0, 0.0),\n",
       " ('2012-10-28 21', 5, 14.0, 0.0),\n",
       " ('2012-10-28 22', 4, 16.0, 0.0),\n",
       " ('2012-10-28 23', 3, 14.0, 0.0),\n",
       " ('2012-10-29 00', 1, 16.0, 0.0),\n",
       " ('2012-10-29 01', 0, 11.0, 0.0),\n",
       " ('2012-10-29 02', 1, 13.0, 0.0),\n",
       " ('2012-10-29 03', 0, 17.0, 0.0),\n",
       " ('2012-10-29 04', 1, 15.0, 0.0),\n",
       " ('2012-10-29 05', 0, 15.0, 0.0),\n",
       " ('2012-10-29 06', 1, 16.0, 0.02),\n",
       " ('2012-10-29 07', 4, 17.0, 0.08),\n",
       " ('2012-10-29 08', 4, 21.0, 0.0),\n",
       " ('2012-10-29 09', 1, 16.0, 0.0),\n",
       " ('2012-10-29 10', 0, None, None),\n",
       " ('2012-10-29 11', 12, 17.5, 0.18),\n",
       " ('2012-10-29 12', 25, 19.2, 1.0000000000000002),\n",
       " ('2012-10-29 13', 12, 26.666666666666668, 0.36),\n",
       " ('2012-10-29 14', 24, 26.75, 0.6),\n",
       " ('2012-10-29 15', 4, 25.0, 0.36),\n",
       " ('2012-10-29 16', 0, 26.0, 0.30000000000000004),\n",
       " ('2012-10-29 17', 8, 21.5, 0.44),\n",
       " ('2012-10-29 18', 6, 23.0, 0.12000000000000001),\n",
       " ('2012-10-29 19', 0, 25.0, 0.01),\n",
       " ('2012-10-29 20', 0, 17.0, 0.0),\n",
       " ('2012-10-29 21', 1, 15.0, 0.0),\n",
       " ('2012-10-29 22', 0, 9.0, 0.02),\n",
       " ('2012-10-29 23', 0, 10.5, 0.18),\n",
       " ('2012-10-30 00', 2, 11.5, 0.05),\n",
       " ('2012-10-30 01', 0, 10.0, 0.0),\n",
       " ('2012-10-30 02', 0, 9.0, 0.03),\n",
       " ('2012-10-30 03', 0, 17.0, 0.04),\n",
       " ('2012-10-30 04', 1, 9.0, 0.0),\n",
       " ('2012-10-30 05', 0, 7.0, 0.01),\n",
       " ('2012-10-30 06', 0, 7.0, 0.01),\n",
       " ('2012-10-30 07', 1, 10.0, 0.0),\n",
       " ('2012-10-30 08', 0, 8.0, 0.060000000000000005),\n",
       " ('2012-10-30 09', 6, 12.0, 0.32),\n",
       " ('2012-10-30 10', 12, 8.0, 0.28),\n",
       " ('2012-10-30 11', 4, 7.0, 0.0),\n",
       " ('2012-10-30 12', 12, 8.0, 0.0),\n",
       " ('2012-10-30 13', 15, 6.0, 0.0),\n",
       " ('2012-10-30 14', 1, None, None),\n",
       " ('2012-10-30 15', 5, None, None),\n",
       " ('2012-10-30 16', 12, 4.666666666666667, 0.11999999999999998),\n",
       " ('2012-10-30 17', 12, 5.5, 0.0),\n",
       " ('2012-10-30 18', 3, 5.0, 0.0),\n",
       " ('2012-10-30 19', 5, 3.0, 0.0),\n",
       " ('2012-10-30 20', 12, 3.5, 0.0),\n",
       " ('2012-10-30 21', 3, 5.0, 0.0),\n",
       " ('2012-10-30 22', 4, 7.0, 0.0),\n",
       " ('2012-10-30 23', 12, 7.0, 0.0),\n",
       " ('2012-10-31 00', 3, 2.6666666666666665, 0.0),\n",
       " ('2012-10-31 01', 0, 5.0, 0.01),\n",
       " ('2012-10-31 02', 3, 0.0, 0.0),\n",
       " ('2012-10-31 03', 1, 8.0, 0.0),\n",
       " ('2012-10-31 04', 1, None, None),\n",
       " ('2012-10-31 05', 0, 0.0, 0.0),\n",
       " ('2012-10-31 06', 3, 6.0, 0.0),\n",
       " ('2012-10-31 07', 4, 8.0, 0.0),\n",
       " ('2012-10-31 08', 12, 7.666666666666667, 0.0),\n",
       " ('2012-10-31 09', 10, 7.0, 0.0),\n",
       " ('2012-10-31 10', 8, 4.0, 0.0),\n",
       " ('2012-10-31 11', 6, 5.0, 0.0),\n",
       " ('2012-10-31 12', 8, 6.0, 0.0),\n",
       " ('2012-10-31 13', 5, 6.0, 0.0),\n",
       " ('2012-10-31 14', 9, 5.0, 0.0),\n",
       " ('2012-10-31 15', 4, 3.0, 0.0),\n",
       " ('2012-10-31 16', 2, 5.0, 0.0),\n",
       " ('2012-10-31 17', 4, 5.0, 0.0),\n",
       " ('2012-10-31 18', 8, 3.0, 0.0),\n",
       " ('2012-10-31 19', 8, 9.0, 0.0),\n",
       " ('2012-10-31 20', 3, 7.0, 0.0),\n",
       " ('2012-10-31 21', 7, 7.0, 0.0),\n",
       " ('2012-10-31 22', 8, 6.0, 0.0),\n",
       " ('2012-10-31 23', 3, 3.0, 0.0),\n",
       " ('2012-11-01 00', 6, 3.0, 0.0),\n",
       " ('2012-11-01 01', 1, 3.0, 0.0),\n",
       " ('2012-11-01 02', 2, 3.0, 0.0),\n",
       " ('2012-11-01 03', 5, None, None),\n",
       " ('2012-11-01 04', 1, 7.0, 0.0),\n",
       " ('2012-11-01 05', 1, 6.0, 0.0),\n",
       " ('2012-11-01 06', 2, 13.0, 0.0),\n",
       " ('2012-11-01 07', 0, None, None),\n",
       " ('2012-11-01 08', 5, 7.0, 0.0),\n",
       " ('2012-11-01 09', 5, 3.0, 0.0),\n",
       " ('2012-11-01 10', 7, 6.0, 0.0),\n",
       " ('2012-11-01 11', 7, 6.0, 0.0),\n",
       " ('2012-11-01 12', 6, 11.0, 0.0),\n",
       " ('2012-11-01 13', 5, 8.0, 0.0),\n",
       " ('2012-11-01 14', 3, 8.0, 0.0),\n",
       " ('2012-11-01 15', 6, None, None),\n",
       " ('2012-11-01 16', 5, 5.0, 0.0),\n",
       " ('2012-11-01 17', 3, 5.0, 0.0),\n",
       " ('2012-11-01 18', 7, 9.0, 0.0),\n",
       " ('2012-11-01 19', 6, 3.0, 0.0),\n",
       " ('2012-11-01 20', 9, 5.0, 0.0),\n",
       " ('2012-11-01 21', 3, 8.0, 0.0),\n",
       " ('2012-11-01 22', 8, 5.0, 0.0),\n",
       " ('2012-11-01 23', 6, 0.0, 0.0),\n",
       " ('2012-11-02 00', 6, 5.0, 0.0),\n",
       " ('2012-11-02 01', 1, 7.0, 0.0),\n",
       " ('2012-11-02 02', 0, 3.0, 0.0),\n",
       " ('2012-11-02 03', 1, 3.0, 0.0),\n",
       " ('2012-11-02 04', 0, 5.0, 0.0),\n",
       " ('2012-11-02 05', 2, 5.0, 0.0),\n",
       " ('2012-11-02 06', 3, 6.0, 0.0),\n",
       " ('2012-11-02 07', 11, None, None),\n",
       " ('2012-11-02 08', 6, 5.0, 0.0),\n",
       " ('2012-11-02 09', 2, 7.0, 0.0),\n",
       " ('2012-11-02 10', 5, 9.0, 0.0),\n",
       " ('2012-11-02 11', 7, 7.0, 0.0),\n",
       " ('2012-11-02 12', 7, 7.0, 0.0),\n",
       " ('2012-11-02 13', 3, 6.0, 0.0),\n",
       " ('2012-11-02 14', 7, 6.0, 0.0),\n",
       " ('2012-11-02 15', 9, 5.0, 0.0),\n",
       " ('2012-11-02 16', 5, 11.0, 0.0),\n",
       " ('2012-11-02 17', 3, 8.0, 0.0),\n",
       " ('2012-11-02 18', 3, 9.0, 0.0),\n",
       " ('2012-11-02 19', 8, 7.0, 0.0),\n",
       " ('2012-11-02 20', 8, 9.0, 0.0),\n",
       " ('2012-11-02 21', 10, 7.0, 0.0),\n",
       " ('2012-11-02 22', 5, 8.0, 0.0),\n",
       " ('2012-11-02 23', 4, 8.0, 0.0),\n",
       " ('2012-11-03 00', 11, 7.0, 0.0),\n",
       " ('2012-11-03 01', 6, 7.0, 0.0),\n",
       " ('2012-11-03 02', 9, 7.0, 0.0),\n",
       " ('2012-11-03 03', 2, 7.0, 0.0),\n",
       " ('2012-11-03 04', 1, 8.0, 0.0),\n",
       " ('2012-11-03 05', 1, 8.0, 0.0),\n",
       " ('2012-11-03 06', 5, 7.0, 0.0),\n",
       " ('2012-11-03 07', 1, 6.0, 0.0),\n",
       " ('2012-11-03 08', 2, 10.0, 0.0),\n",
       " ('2012-11-03 09', 5, 13.0, 0.0),\n",
       " ('2012-11-03 10', 4, 6.0, 0.0),\n",
       " ('2012-11-03 11', 16, 13.0, 0.0),\n",
       " ('2012-11-03 12', 4, 13.0, 0.0),\n",
       " ('2012-11-03 13', 4, 8.0, 0.0),\n",
       " ('2012-11-03 14', 12, 8.0, 0.0),\n",
       " ('2012-11-03 15', 5, 7.0, 0.0),\n",
       " ('2012-11-03 16', 3, 10.0, 0.0),\n",
       " ('2012-11-03 17', 14, 9.0, 0.0),\n",
       " ('2012-11-03 18', 10, 9.0, 0.0),\n",
       " ('2012-11-03 19', 8, 13.0, 0.0),\n",
       " ('2012-11-03 20', 14, 10.0, 0.0),\n",
       " ('2012-11-03 21', 15, 9.0, 0.0),\n",
       " ('2012-11-03 22', 14, 0.0, 0.0),\n",
       " ('2012-11-03 23', 12, 7.0, 0.0),\n",
       " ('2012-11-04 00', 11, 9.0, 0.0),\n",
       " ('2012-11-04 01', 16, 7.0, 0.0),\n",
       " ('2012-11-04 02', 16, 7.0, 0.0),\n",
       " ('2012-11-04 03', 3, 7.0, 0.0),\n",
       " ('2012-11-04 04', 4, 8.0, 0.0),\n",
       " ('2012-11-04 05', 3, 6.0, 0.0),\n",
       " ('2012-11-04 06', 0, 6.0, 0.0),\n",
       " ('2012-11-04 07', 3, 3.0, 0.0),\n",
       " ('2012-11-04 08', 6, 7.0, 0.0),\n",
       " ('2012-11-04 09', 2, 9.0, 0.0),\n",
       " ('2012-11-04 10', 4, None, None),\n",
       " ('2012-11-04 11', 9, 6.0, 0.0),\n",
       " ('2012-11-04 12', 12, 8.0, 0.0),\n",
       " ('2012-11-04 13', 9, 8.0, 0.0),\n",
       " ('2012-11-04 14', 5, 7.0, 0.0),\n",
       " ('2012-11-04 15', 3, 7.0, 0.0),\n",
       " ('2012-11-04 16', 6, 5.0, 0.0),\n",
       " ('2012-11-04 17', 12, 5.0, 0.0),\n",
       " ('2012-11-04 18', 8, None, None),\n",
       " ('2012-11-04 19', 11, 7.0, 0.0),\n",
       " ('2012-11-04 20', 9, None, None),\n",
       " ('2012-11-04 21', 7, 7.0, 0.0),\n",
       " ('2012-11-04 22', 5, 6.0, 0.0),\n",
       " ('2012-11-04 23', 1, 5.0, 0.0),\n",
       " ('2012-11-05 00', 2, 0.0, 0.0),\n",
       " ('2012-11-05 01', 2, 5.0, 0.0),\n",
       " ('2012-11-05 02', 1, 3.0, 0.0),\n",
       " ('2012-11-05 03', 0, 7.0, 0.0),\n",
       " ('2012-11-05 04', 3, 3.0, 0.0),\n",
       " ('2012-11-05 05', 1, 6.0, 0.0),\n",
       " ('2012-11-05 06', 6, 8.0, 0.0),\n",
       " ('2012-11-05 07', 4, 6.0, 0.0),\n",
       " ('2012-11-05 08', 14, 7.0, 0.0),\n",
       " ('2012-11-05 09', 6, 3.0, 0.0),\n",
       " ('2012-11-05 10', 9, 3.0, 0.0),\n",
       " ('2012-11-05 11', 9, 3.0, 0.0),\n",
       " ('2012-11-05 12', 5, 5.0, 0.0),\n",
       " ('2012-11-05 13', 5, 3.0, 0.0),\n",
       " ('2012-11-05 14', 10, None, None),\n",
       " ('2012-11-05 15', 10, 8.0, 0.0),\n",
       " ('2012-11-05 16', 4, None, None),\n",
       " ('2012-11-05 17', 16, 5.0, 0.0),\n",
       " ('2012-11-05 18', 11, 5.0, 0.0),\n",
       " ('2012-11-05 19', 11, 0.0, 0.0),\n",
       " ('2012-11-05 20', 13, 3.0, 0.0),\n",
       " ('2012-11-05 21', 9, 7.0, 0.0),\n",
       " ('2012-11-05 22', 8, 6.0, 0.0),\n",
       " ('2012-11-05 23', 6, 9.0, 0.0),\n",
       " ('2012-11-06 00', 9, 6.0, 0.0),\n",
       " ('2012-11-06 01', 4, 5.0, 0.0),\n",
       " ('2012-11-06 02', 2, 8.0, 0.0),\n",
       " ('2012-11-06 03', 1, 10.0, 0.0),\n",
       " ('2012-11-06 04', 2, 6.0, 0.0),\n",
       " ('2012-11-06 05', 2, 5.0, 0.0),\n",
       " ('2012-11-06 06', 2, 7.0, 0.0),\n",
       " ('2012-11-06 07', 7, 7.0, 0.0),\n",
       " ('2012-11-06 08', 8, 8.0, 0.0),\n",
       " ('2012-11-06 09', 4, 3.0, 0.0),\n",
       " ('2012-11-06 10', 12, 6.0, 0.0),\n",
       " ('2012-11-06 11', 4, 7.0, 0.0),\n",
       " ('2012-11-06 12', 7, 5.0, 0.0),\n",
       " ('2012-11-06 13', 11, 0.0, 0.0),\n",
       " ('2012-11-06 14', 10, 6.0, 0.0),\n",
       " ('2012-11-06 15', 6, 6.0, 0.0),\n",
       " ('2012-11-06 16', 6, 5.0, 0.0),\n",
       " ('2012-11-06 17', 9, 5.0, 0.0),\n",
       " ('2012-11-06 18', 12, 3.0, 0.0),\n",
       " ('2012-11-06 19', 14, 3.0, 0.0),\n",
       " ('2012-11-06 20', 11, 7.0, 0.0),\n",
       " ('2012-11-06 21', 7, 7.0, 0.0),\n",
       " ('2012-11-06 22', 7, 7.0, 0.0),\n",
       " ('2012-11-06 23', 7, 3.0, 0.0),\n",
       " ('2012-11-07 00', 0, 9.0, 0.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the query\n",
    "engine.execute(QUERY_STMT_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "431b73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query to file\n",
    "query_to_file(QUERY_STMT_6, query_file_6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
