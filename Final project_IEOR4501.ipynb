{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49b47bf",
   "metadata": {},
   "source": [
    "## Part 0: Setup\n",
    "In this part we set up some basic environment and variables needed for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe6aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import math\n",
    "import requests\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keplergl as kg\n",
    "from scipy import stats\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed8c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables needed\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV = [\"weather-2009.csv\", \"weather-2010.csv\", \"weather-2011.csv\", \n",
    "               \"weather-2012.csv\", \"weather-2013.csv\", \"weather-2014.csv\", \"weather-2015.csv\"]\n",
    "ZONE_PATH = \"taxi_zones.shp\"\n",
    "\n",
    "NY_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE = \"sqlite:///project.db\"\n",
    "SCHEMA_FILE = \"schema.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1366081",
   "metadata": {},
   "outputs": [],
   "source": [
    "Taxi_zone = gpd.read_file(ZONE_PATH)\n",
    "Taxi_zone = Taxi_zone.to_crs(4326)\n",
    "Taxi_zone['longitude'] = Taxi_zone.centroid.x  \n",
    "Taxi_zone['latitude'] = Taxi_zone.centroid.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d022e5",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "### Yellow Taxi trip data: Downloading, Cleaning, Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b889eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_html() -> bytes:\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019bea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_links() -> list:\n",
    "    links = []\n",
    "    pattern = r\"yellow_tripdata_2009|yellow_tripdata_201[0-4]|yellow_tripdata_2015-0[1-6]\"\n",
    "    soup = bs4.BeautifulSoup(get_taxi_html(),'html.parser')\n",
    "    for a in soup.find_all(\"a\",href = True):\n",
    "        link_text = a.get(\"href\")\n",
    "        matches = re.findall(pattern,link_text)\n",
    "        if matches:\n",
    "            links.append(link_text)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef193e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_taxi_data_download_clean_sample(url: str) -> pd.core.frame.DataFrame:\n",
    "    parquet_name = url.split(\"/\")[-1]\n",
    "\n",
    "    # download if it doesn't exist\n",
    "    if not os.path.exists(parquet_name):\n",
    "        print(f\"Downloading parquet for {parquet_name[16:23]}.\")\n",
    "        file = requests.get(url)\n",
    "        with open(parquet_name , \"wb\") as f:\n",
    "            f.write(file.content)\n",
    "    \n",
    "    # load data from parquet file\n",
    "    data = pd.read_parquet(parquet_name)\n",
    "    print(f\"Cleaning data for {parquet_name[16:23]}.\")\n",
    "    \n",
    "    # 为了不占用太多内存，读一个删一个，最后提交之前要删掉\n",
    "    os.remove(parquet_name) \n",
    "    print(f\"Parquet for {parquet_name[16:23]} is removed.\")\n",
    "    \n",
    "    # looking up the latitude and longitude for some months where only location IDs are given for pickups and dropoffs\n",
    "    # keep NaNs if exists\n",
    "    if \"PULocationID\" in data.columns:\n",
    "        data[\"pickup_latitude\"] = data[\"PULocationID\"].map(Taxi_zone[\"latitude\"], na_action = \"ignore\")\n",
    "        data[\"pickup_longitude\"] = data[\"PULocationID\"].map(Taxi_zone[\"longitude\"], na_action = \"ignore\")\n",
    "        data[\"dropoff_latitude\"] = data[\"DOLocationID\"].map(Taxi_zone[\"latitude\"], na_action = \"ignore\")\n",
    "        data[\"dropoff_longitude\"] = data[\"DOLocationID\"].map(Taxi_zone[\"longitude\"], na_action = \"ignore\")\n",
    "    \n",
    "    # normalize column names\n",
    "    rename_dict = {\n",
    "        \"VendorID\" : \"vendor_id\",\n",
    "        \"tpep_pickup_datetime\" : \"pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\" : \"dropoff_datetime\",\n",
    "        \"RatecodeID\" : \"rate_code\",\n",
    "        \"Trip_Pickup_DateTime\" : \"pickup_datetime\",\n",
    "        \"Trip_Dropoff_DateTime\" : \"dropoff_datetime\",\n",
    "        \"Start_Lon\" : \"pickup_longitude\",\n",
    "        \"Start_Lat\" : \"pickup_latitude\",\n",
    "        \"End_Lon\" : \"dropoff_longitude\",\n",
    "        \"End_Lat\" : \"dropoff_latitude\",\n",
    "        \"Fare_Amt\" : \"fare_amount\",\n",
    "        \"Tip_Amt\" : \"tip_amount\",\n",
    "        \"Tolls_Amt\" : \"tolls_amount\",\n",
    "        \"Total_Amt\" : \"total_amount\"\n",
    "    }\n",
    "    data.rename(columns = rename_dict, inplace = True)\n",
    "    \n",
    "    # remove the trips that the location IDs are be valid\n",
    "    data.dropna(subset=[\"pickup_latitude\",\"pickup_longitude\",\"dropoff_latitude\",\"dropoff_longitude\"],inplace = True)\n",
    "    \n",
    "    # remove invalid data points\n",
    "    data = data[data[\"total_amount\"] > 0]\n",
    "    \n",
    "    # normalize and use appropriate column types for the respective data\n",
    "    data[\"pickup_datetime\"] = pd.to_datetime(data[\"pickup_datetime\"])\n",
    "    data[\"dropoff_datetime\"] = pd.to_datetime(data[\"dropoff_datetime\"])\n",
    "    data = data.astype({\"pickup_latitude\": \"float64\",\"pickup_longitude\": \"float64\",\\\n",
    "                        \"dropoff_latitude\": \"float64\",\"dropoff_longitude\": \"float64\",\"tip_amount\": \"float64\"})\n",
    "    \n",
    "    # remove unnecessary columns and only keeping columns needed\n",
    "    data = data[[\"pickup_datetime\",\"pickup_latitude\",\"pickup_longitude\",\"dropoff_latitude\",\"dropoff_longitude\",\"tip_amount\"]]\n",
    "    \n",
    "    # remove trips that start and/or end outside of NY\n",
    "    data = data[(data[\"pickup_latitude\"] >= NY_COORDS[0][0]) & (data[\"pickup_latitude\"] <= NY_COORDS[1][0])]\n",
    "    data = data[(data[\"pickup_longitude\"] >= NY_COORDS[0][1]) & (data[\"pickup_longitude\"] <= NY_COORDS[1][1])]\n",
    "    data = data[(data[\"dropoff_latitude\"] >= NY_COORDS[0][0]) & (data[\"dropoff_latitude\"] <= NY_COORDS[1][0])]\n",
    "    data = data[(data[\"dropoff_longitude\"] >= NY_COORDS[0][1]) & (data[\"dropoff_longitude\"] <= NY_COORDS[1][1])]\n",
    "    \n",
    "    # Sampling\n",
    "    # Uber dataset consists of 200000 data points\n",
    "    # Therefore, we need 200000/78 ~ 2564 data points from each month\n",
    "    data = data.sample(2564)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcc595",
   "metadata": {},
   "source": [
    "### Yellow Taxi trip data: Filling (Distance)\n",
    "\n",
    "We calculate the distance between pickup location and dropoff location using the Haversine Formula:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/2789198/27240436-e9a459da-52d4-11e7-8f84-f96d0b312859.png)\n",
    "\n",
    "where $\\lambda$ and $\\phi$ are the `longitude` and `latitude` of locations respectively, $r$ is the radius of earth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd6abe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(pu_coord: pd.core.frame.DataFrame, do_coord: pd.core.frame.DataFrame) -> pd.core.series.Series:\n",
    "    \n",
    "    pick_lon = pu_coord[\"pickup_longitude\"].map(math.radians)\n",
    "    pick_lat = pu_coord[\"pickup_latitude\"].map(math.radians)\n",
    "    drop_lon = do_coord[\"dropoff_longitude\"].map(math.radians)\n",
    "    drop_lat = do_coord[\"dropoff_latitude\"].map(math.radians)\n",
    "    \n",
    "    delta_lat = drop_lat - pick_lat\n",
    "    delta_lon = drop_lon - pick_lon\n",
    "    \n",
    "    # Take the average earth radius (km) as r\n",
    "    r = 6371\n",
    "    part_formula = ((delta_lat/2).map(math.sin))**2 + (pick_lat.map(math.cos))*(drop_lat.map(math.cos))*((delta_lon/2).map(math.sin))**2\n",
    "    dist = 2 * r * part_formula.map(math.sqrt).map(math.asin)\n",
    "    \n",
    "    return dist.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547f8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filling_distance(data: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    pu_coord = data[[\"pickup_longitude\",\"pickup_latitude\"]]\n",
    "    do_coord = data[[\"dropoff_longitude\",\"dropoff_latitude\"]]\n",
    "    data[\"distance\"] = calculate_distance(pu_coord, do_coord)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66491c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_taxi_data(urls: list) -> pd.core.frame.DataFrame:\n",
    "    all_taxi_df = []\n",
    "    for url in urls:\n",
    "        data = monthly_taxi_data_download_clean_sample(url)\n",
    "        data = filling_distance(data)\n",
    "        all_taxi_df.append(data)\n",
    "    \n",
    "    all_data = pd.concat(all_taxi_df)\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaccf37c",
   "metadata": {},
   "source": [
    "### Uber rides data: Reading, Cleaning and Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12569d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uber_data_read_clean_fill() -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    data = pd.read_csv(UBER_CSV, low_memory = False)\n",
    "    print(\"Cleaning data for Uber rides.\")\n",
    "    \n",
    "    # remove the trips that the location IDs are be valid\n",
    "    data.dropna(subset=[\"pickup_latitude\",\"pickup_longitude\",\"dropoff_latitude\",\"dropoff_longitude\"],inplace = True)\n",
    "    \n",
    "    # normalize and use appropriate column types for the respective data\n",
    "    data[\"pickup_datetime\"] = pd.to_datetime(data[\"pickup_datetime\"])\n",
    "    data = data.astype({\"pickup_latitude\": \"float64\",\"pickup_longitude\": \"float64\",\\\n",
    "                        \"dropoff_latitude\": \"float64\",\"dropoff_longitude\": \"float64\"})\n",
    "    \n",
    "    # remove invalid data points\n",
    "    data = data[data[\"fare_amount\"] > 0]\n",
    "    \n",
    "    # remove unnecessary columns and only keeping columns needed \n",
    "    data = data[[\"pickup_datetime\",\"pickup_latitude\",\"pickup_longitude\",\"dropoff_latitude\",\"dropoff_longitude\"]]\n",
    "    \n",
    "    # remove trips that start and/or end outside of NY\n",
    "    data = data[(data[\"pickup_latitude\"] >= NY_COORDS[0][0]) & (data[\"pickup_latitude\"] <= NY_COORDS[1][0])]\n",
    "    data = data[(data[\"pickup_longitude\"] >= NY_COORDS[0][1]) & (data[\"pickup_longitude\"] <= NY_COORDS[1][1])]\n",
    "    data = data[(data[\"dropoff_latitude\"] >= NY_COORDS[0][0]) & (data[\"dropoff_latitude\"] <= NY_COORDS[1][0])]\n",
    "    data = data[(data[\"dropoff_longitude\"] >= NY_COORDS[0][1]) & (data[\"dropoff_longitude\"] <= NY_COORDS[1][1])]\n",
    "    \n",
    "    # fill in distance column\n",
    "    data = filling_distance(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f7396",
   "metadata": {},
   "source": [
    "### Weather data: Reading, Cleaning and Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "965f99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_weather_data_read_clean_fill(csv: str) -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    data = pd.read_csv(csv, low_memory = False)\n",
    "    \n",
    "    # remove columns of no use\n",
    "    data = data[[\"DATE\",\"HourlyWindSpeed\",\"HourlyPrecipitation\"]]\n",
    "\n",
    "    # remove missing values for wind speed data\n",
    "    data.dropna(subset=[\"HourlyWindSpeed\"], inplace=True)\n",
    "\n",
    "    # normalize and use appropriate column types for the respective data\n",
    "    data[\"DATE\"] = pd.to_datetime(data[\"DATE\"])\n",
    "    data[\"HourlyPrecipitation\"] = pd.to_numeric(data[\"HourlyPrecipitation\"], errors = \"coerce\")\n",
    "\n",
    "    # fill 0 to NAs in the precipitation data\n",
    "    data[\"HourlyPrecipitation\"].fillna(0, inplace=True)\n",
    "    data = data.astype({\"HourlyWindSpeed\":\"float64\", \"HourlyPrecipitation\":\"float64\"})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3629e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_weather_data_read_clean_fill(csv: str) -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    data = pd.read_csv(csv, low_memory = False)\n",
    "    data[\"DATE\"] = pd.to_datetime(data[\"DATE\"])\n",
    "    data[\"HourlyPrecipitation\"] = pd.to_numeric(data[\"HourlyPrecipitation\"], errors = \"coerce\")\n",
    "    data[\"HourlyPrecipitation\"].fillna(0, inplace = True)\n",
    "    \n",
    "    # Only take date into consideration now\n",
    "    data[\"DATE\"] = data[\"DATE\"].dt.date\n",
    "    data_daily = data.groupby([\"DATE\"],as_index = False).agg({\"HourlyPrecipitation\":\"sum\",\"HourlyWindSpeed\":\"mean\"})\n",
    "    data_daily.rename(columns = {\"HourlyPrecipitation\" : \"Precipitation\", \"HourlyWindSpeed\" : \"WindSpeed\"}, inplace = True)\n",
    "    \n",
    "    \n",
    "    data_daily[\"WindSpeed\"].round(2)\n",
    "    data_daily[\"DATE\"] = pd.to_datetime(data_daily[\"DATE\"])\n",
    "    data_daily = data_daily.astype({\"WindSpeed\":\"float64\", \"Precipitation\":\"float64\"})\n",
    "    \n",
    "    return data_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cad9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_sun_data_read(csv: str) -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    data = pd.read_csv(csv, low_memory = False)\n",
    "    data[\"DATE\"] = pd.to_datetime(data[\"DATE\"]).dt.date\n",
    "    data_sun = data.groupby([\"DATE\"], as_index = False).agg({\"Sunrise\":\"first\",\"Sunset\":\"first\"})\n",
    "    data_sun = data_sun.dropna()\n",
    "    data_sun[\"DATE\"] = pd.to_datetime(data_sun[\"DATE\"])\n",
    "    \n",
    "    # use appropriate column types for the respective data\n",
    "    data_sun = data_sun.astype({\"Sunrise\":\"int32\", \"Sunset\":\"int32\"})\n",
    "    data_sun = data_sun.astype({\"Sunrise\":\"string\", \"Sunset\":\"string\"})\n",
    "    \n",
    "    return data_sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f6e119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_weather_data() -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    hourly_data = []\n",
    "    daily_data = []\n",
    "    sun_data = []\n",
    "    \n",
    "    for csv in WEATHER_CSV:\n",
    "        ho_data = hourly_weather_data_read_clean_fill(csv)\n",
    "        da_data = daily_weather_data_read_clean_fill(csv)\n",
    "        su_data = daily_sun_data_read(csv)\n",
    "        \n",
    "        # for year 2015, only need data for the first six month\n",
    "        if csv == \"weather-2015.csv\":\n",
    "            ho_data = ho_data[ho_data[\"DATE\"].dt.month <= 6]\n",
    "            da_data = da_data[da_data[\"DATE\"].dt.month <= 6]\n",
    "            su_data = su_data[su_data[\"DATE\"].dt.month <= 6]\n",
    "            \n",
    "        hourly_data.append(ho_data)\n",
    "        daily_data.append(da_data)\n",
    "        sun_data.append(su_data)\n",
    "    \n",
    "    hour_data = pd.concat(hourly_data)\n",
    "    day_data = pd.concat(daily_data)\n",
    "    day_sun_data = pd.concat(sun_data)\n",
    "    \n",
    "    return hour_data, day_data, day_sun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc49e7",
   "metadata": {},
   "source": [
    "### Process all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e6df801",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data, daily_sun_data = all_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e5f6a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data for Uber rides.\n"
     ]
    }
   ],
   "source": [
    "uber_data = uber_data_read_clean_fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b86ebd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading parquet for 2015-01.\n",
      "Cleaning data for 2015-01.\n",
      "Parquet for 2015-01 is removed.\n",
      "Downloading parquet for 2015-02.\n",
      "Cleaning data for 2015-02.\n",
      "Parquet for 2015-02 is removed.\n",
      "Downloading parquet for 2015-03.\n",
      "Cleaning data for 2015-03.\n",
      "Parquet for 2015-03 is removed.\n",
      "Downloading parquet for 2015-04.\n",
      "Cleaning data for 2015-04.\n",
      "Parquet for 2015-04 is removed.\n",
      "Downloading parquet for 2015-05.\n",
      "Cleaning data for 2015-05.\n",
      "Parquet for 2015-05 is removed.\n",
      "Downloading parquet for 2015-06.\n",
      "Cleaning data for 2015-06.\n",
      "Parquet for 2015-06 is removed.\n",
      "Downloading parquet for 2014-01.\n",
      "Cleaning data for 2014-01.\n",
      "Parquet for 2014-01 is removed.\n",
      "Downloading parquet for 2014-02.\n",
      "Cleaning data for 2014-02.\n",
      "Parquet for 2014-02 is removed.\n",
      "Downloading parquet for 2014-03.\n",
      "Cleaning data for 2014-03.\n",
      "Parquet for 2014-03 is removed.\n",
      "Downloading parquet for 2014-04.\n",
      "Cleaning data for 2014-04.\n",
      "Parquet for 2014-04 is removed.\n",
      "Downloading parquet for 2014-05.\n",
      "Cleaning data for 2014-05.\n",
      "Parquet for 2014-05 is removed.\n",
      "Downloading parquet for 2014-06.\n",
      "Cleaning data for 2014-06.\n",
      "Parquet for 2014-06 is removed.\n",
      "Downloading parquet for 2014-07.\n",
      "Cleaning data for 2014-07.\n",
      "Parquet for 2014-07 is removed.\n",
      "Downloading parquet for 2014-08.\n",
      "Cleaning data for 2014-08.\n",
      "Parquet for 2014-08 is removed.\n",
      "Downloading parquet for 2014-09.\n",
      "Cleaning data for 2014-09.\n",
      "Parquet for 2014-09 is removed.\n",
      "Downloading parquet for 2014-10.\n",
      "Cleaning data for 2014-10.\n",
      "Parquet for 2014-10 is removed.\n",
      "Downloading parquet for 2014-11.\n",
      "Cleaning data for 2014-11.\n",
      "Parquet for 2014-11 is removed.\n",
      "Downloading parquet for 2014-12.\n",
      "Cleaning data for 2014-12.\n",
      "Parquet for 2014-12 is removed.\n",
      "Downloading parquet for 2013-01.\n",
      "Cleaning data for 2013-01.\n",
      "Parquet for 2013-01 is removed.\n",
      "Downloading parquet for 2013-02.\n",
      "Cleaning data for 2013-02.\n",
      "Parquet for 2013-02 is removed.\n",
      "Downloading parquet for 2013-03.\n",
      "Cleaning data for 2013-03.\n",
      "Parquet for 2013-03 is removed.\n",
      "Downloading parquet for 2013-04.\n",
      "Cleaning data for 2013-04.\n",
      "Parquet for 2013-04 is removed.\n",
      "Downloading parquet for 2013-05.\n",
      "Cleaning data for 2013-05.\n",
      "Parquet for 2013-05 is removed.\n",
      "Downloading parquet for 2013-06.\n",
      "Cleaning data for 2013-06.\n",
      "Parquet for 2013-06 is removed.\n",
      "Downloading parquet for 2013-07.\n",
      "Cleaning data for 2013-07.\n",
      "Parquet for 2013-07 is removed.\n",
      "Downloading parquet for 2013-08.\n",
      "Cleaning data for 2013-08.\n",
      "Parquet for 2013-08 is removed.\n",
      "Downloading parquet for 2013-09.\n",
      "Cleaning data for 2013-09.\n",
      "Parquet for 2013-09 is removed.\n",
      "Downloading parquet for 2013-10.\n",
      "Cleaning data for 2013-10.\n",
      "Parquet for 2013-10 is removed.\n",
      "Downloading parquet for 2013-11.\n",
      "Cleaning data for 2013-11.\n",
      "Parquet for 2013-11 is removed.\n",
      "Downloading parquet for 2013-12.\n",
      "Cleaning data for 2013-12.\n",
      "Parquet for 2013-12 is removed.\n",
      "Downloading parquet for 2012-01.\n",
      "Cleaning data for 2012-01.\n",
      "Parquet for 2012-01 is removed.\n",
      "Downloading parquet for 2012-02.\n",
      "Cleaning data for 2012-02.\n",
      "Parquet for 2012-02 is removed.\n",
      "Downloading parquet for 2012-03.\n",
      "Cleaning data for 2012-03.\n",
      "Parquet for 2012-03 is removed.\n",
      "Downloading parquet for 2012-04.\n",
      "Cleaning data for 2012-04.\n",
      "Parquet for 2012-04 is removed.\n",
      "Downloading parquet for 2012-05.\n",
      "Cleaning data for 2012-05.\n",
      "Parquet for 2012-05 is removed.\n",
      "Downloading parquet for 2012-06.\n",
      "Cleaning data for 2012-06.\n",
      "Parquet for 2012-06 is removed.\n",
      "Downloading parquet for 2012-07.\n",
      "Cleaning data for 2012-07.\n",
      "Parquet for 2012-07 is removed.\n",
      "Downloading parquet for 2012-08.\n",
      "Cleaning data for 2012-08.\n",
      "Parquet for 2012-08 is removed.\n",
      "Downloading parquet for 2012-09.\n",
      "Cleaning data for 2012-09.\n",
      "Parquet for 2012-09 is removed.\n",
      "Downloading parquet for 2012-10.\n",
      "Cleaning data for 2012-10.\n",
      "Parquet for 2012-10 is removed.\n",
      "Downloading parquet for 2012-11.\n",
      "Cleaning data for 2012-11.\n",
      "Parquet for 2012-11 is removed.\n",
      "Downloading parquet for 2012-12.\n",
      "Cleaning data for 2012-12.\n",
      "Parquet for 2012-12 is removed.\n",
      "Downloading parquet for 2011-01.\n",
      "Cleaning data for 2011-01.\n",
      "Parquet for 2011-01 is removed.\n",
      "Downloading parquet for 2011-02.\n",
      "Cleaning data for 2011-02.\n",
      "Parquet for 2011-02 is removed.\n",
      "Downloading parquet for 2011-03.\n",
      "Cleaning data for 2011-03.\n",
      "Parquet for 2011-03 is removed.\n",
      "Downloading parquet for 2011-04.\n",
      "Cleaning data for 2011-04.\n",
      "Parquet for 2011-04 is removed.\n",
      "Downloading parquet for 2011-05.\n",
      "Cleaning data for 2011-05.\n",
      "Parquet for 2011-05 is removed.\n",
      "Downloading parquet for 2011-06.\n",
      "Cleaning data for 2011-06.\n",
      "Parquet for 2011-06 is removed.\n",
      "Downloading parquet for 2011-07.\n",
      "Cleaning data for 2011-07.\n",
      "Parquet for 2011-07 is removed.\n",
      "Downloading parquet for 2011-08.\n",
      "Cleaning data for 2011-08.\n",
      "Parquet for 2011-08 is removed.\n",
      "Downloading parquet for 2011-09.\n",
      "Cleaning data for 2011-09.\n",
      "Parquet for 2011-09 is removed.\n",
      "Downloading parquet for 2011-10.\n",
      "Cleaning data for 2011-10.\n",
      "Parquet for 2011-10 is removed.\n",
      "Downloading parquet for 2011-11.\n",
      "Cleaning data for 2011-11.\n",
      "Parquet for 2011-11 is removed.\n",
      "Downloading parquet for 2011-12.\n",
      "Cleaning data for 2011-12.\n",
      "Parquet for 2011-12 is removed.\n",
      "Downloading parquet for 2010-01.\n",
      "Cleaning data for 2010-01.\n",
      "Parquet for 2010-01 is removed.\n",
      "Downloading parquet for 2010-02.\n",
      "Cleaning data for 2010-02.\n",
      "Parquet for 2010-02 is removed.\n",
      "Downloading parquet for 2010-03.\n",
      "Cleaning data for 2010-03.\n",
      "Parquet for 2010-03 is removed.\n",
      "Downloading parquet for 2010-04.\n",
      "Cleaning data for 2010-04.\n",
      "Parquet for 2010-04 is removed.\n",
      "Downloading parquet for 2010-05.\n",
      "Cleaning data for 2010-05.\n",
      "Parquet for 2010-05 is removed.\n",
      "Downloading parquet for 2010-06.\n",
      "Cleaning data for 2010-06.\n",
      "Parquet for 2010-06 is removed.\n",
      "Downloading parquet for 2010-07.\n",
      "Cleaning data for 2010-07.\n",
      "Parquet for 2010-07 is removed.\n",
      "Downloading parquet for 2010-08.\n",
      "Cleaning data for 2010-08.\n",
      "Parquet for 2010-08 is removed.\n",
      "Downloading parquet for 2010-09.\n",
      "Cleaning data for 2010-09.\n",
      "Parquet for 2010-09 is removed.\n",
      "Downloading parquet for 2010-10.\n",
      "Cleaning data for 2010-10.\n",
      "Parquet for 2010-10 is removed.\n",
      "Downloading parquet for 2010-11.\n",
      "Cleaning data for 2010-11.\n",
      "Parquet for 2010-11 is removed.\n",
      "Downloading parquet for 2010-12.\n",
      "Cleaning data for 2010-12.\n",
      "Parquet for 2010-12 is removed.\n",
      "Downloading parquet for 2009-01.\n",
      "Cleaning data for 2009-01.\n",
      "Parquet for 2009-01 is removed.\n",
      "Downloading parquet for 2009-02.\n",
      "Cleaning data for 2009-02.\n",
      "Parquet for 2009-02 is removed.\n",
      "Downloading parquet for 2009-03.\n",
      "Cleaning data for 2009-03.\n",
      "Parquet for 2009-03 is removed.\n",
      "Downloading parquet for 2009-04.\n",
      "Cleaning data for 2009-04.\n",
      "Parquet for 2009-04 is removed.\n",
      "Downloading parquet for 2009-05.\n",
      "Cleaning data for 2009-05.\n",
      "Parquet for 2009-05 is removed.\n",
      "Downloading parquet for 2009-06.\n",
      "Cleaning data for 2009-06.\n",
      "Parquet for 2009-06 is removed.\n",
      "Downloading parquet for 2009-07.\n",
      "Cleaning data for 2009-07.\n",
      "Parquet for 2009-07 is removed.\n",
      "Downloading parquet for 2009-08.\n",
      "Cleaning data for 2009-08.\n",
      "Parquet for 2009-08 is removed.\n",
      "Downloading parquet for 2009-09.\n",
      "Cleaning data for 2009-09.\n",
      "Parquet for 2009-09 is removed.\n",
      "Downloading parquet for 2009-10.\n",
      "Cleaning data for 2009-10.\n",
      "Parquet for 2009-10 is removed.\n",
      "Downloading parquet for 2009-11.\n",
      "Cleaning data for 2009-11.\n",
      "Parquet for 2009-11 is removed.\n",
      "Downloading parquet for 2009-12.\n",
      "Cleaning data for 2009-12.\n",
      "Parquet for 2009-12 is removed.\n"
     ]
    }
   ],
   "source": [
    "links = find_taxi_parquet_links()\n",
    "taxi_data = all_taxi_data(links) # 加了个范围"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293dd77",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data\n",
    "### Create database and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "825ea545",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1327c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define statements to create tables\n",
    "\n",
    "Taxi_STMT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trip\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_latitude FLOAT64,\n",
    "    pickup_longitude FLOAT64,\n",
    "    dropoff_latitude FLOAT64,\n",
    "    dropoff_longitude FLOAT64,\n",
    "    tip_amount FLOAT64,\n",
    "    distance FLOAT64\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "Uber_STMT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trip\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_latitude FLOAT64,\n",
    "    pickup_longitude FLOAT64,\n",
    "    dropoff_latitude FLOAT64,\n",
    "    dropoff_longitude FLOAT64,\n",
    "    distance FLOAT64\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "Hourly_Weather_STMT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    HourlyWindSpeed FLOAT64,\n",
    "    HourlyPrecipitation FLOAT64\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "Daily_Weather_STMT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    Precipitation FLOAT64,\n",
    "    WindSpeed FLOAT64\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "Sun_STMT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_sun\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    Sunrise STRING,\n",
    "    Sunset STRING\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17b2e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(Taxi_STMT)\n",
    "    connection.execute(Uber_STMT)\n",
    "    connection.execute(Hourly_Weather_STMT)\n",
    "    connection.execute(Daily_Weather_STMT)\n",
    "    connection.execute(Sun_STMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48143f15",
   "metadata": {},
   "source": [
    "### Add cleaned data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5f76a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi_data added.\n",
      "Uber_data added.\n",
      "Hourly_weather_data added.\n",
      "Daily_weather_data added.\n",
      "Sun_data added.\n"
     ]
    }
   ],
   "source": [
    "# add data from the dataframes to corresponding SQL tables\n",
    "taxi_data.to_sql(\"taxi_trip\", engine, if_exists = \"replace\", index = False)\n",
    "print(\"Taxi_data added.\")\n",
    "uber_data.to_sql(\"uber_trip\", engine, if_exists = \"replace\", index = False)\n",
    "print(\"Uber_data added.\")\n",
    "hourly_weather_data.to_sql(\"hourly_weather\", engine, if_exists = \"replace\", index = False)\n",
    "print(\"Hourly_weather_data added.\")\n",
    "daily_weather_data.to_sql(\"daily_weather\", engine, if_exists = \"replace\", index = False)\n",
    "print(\"Daily_weather_data added.\")\n",
    "daily_sun_data.to_sql(\"daily_sun\", engine, if_exists = \"replace\", index = False)\n",
    "print(\"Sun_data added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1bbf2",
   "metadata": {},
   "source": [
    "### Create a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c331ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(Taxi_STMT)\n",
    "    f.write(Uber_STMT)\n",
    "    f.write(Hourly_Weather_STMT)\n",
    "    f.write(Daily_Weather_STMT)\n",
    "    f.write(Sun_STMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41e4df",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Data\n",
    "First of all, for simplicity, define a function to store the query files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b94edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_file(query: str, file_name: str) -> None:\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9db6d8",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "For 01-2009 through 06-2015, what hour of the day was the most popular to take a Yellow Taxi? The result should have 24 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3762b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query_file name for question 1\n",
    "query_file_1 = \"popular_hour_for_taxi.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65aa6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query\n",
    "QUERY_STMT_1 = \"\"\"\n",
    "SELECT strftime (\"%H\",pickup_datetime) AS hour, COUNT(*) AS counts\n",
    "FROM taxi_trip\n",
    "GROUP BY hour\n",
    "ORDER BY counts DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05f2d487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('19', 12490),\n",
       " ('18', 12179),\n",
       " ('20', 11799),\n",
       " ('21', 11507),\n",
       " ('22', 11070),\n",
       " ('23', 10022),\n",
       " ('14', 9948),\n",
       " ('17', 9881),\n",
       " ('12', 9777),\n",
       " ('15', 9604),\n",
       " ('13', 9604),\n",
       " ('09', 9339),\n",
       " ('11', 9237),\n",
       " ('10', 9105),\n",
       " ('08', 9053),\n",
       " ('16', 8267),\n",
       " ('00', 7921),\n",
       " ('07', 7305),\n",
       " ('01', 5919),\n",
       " ('02', 4455),\n",
       " ('06', 4047),\n",
       " ('03', 3223),\n",
       " ('04', 2326),\n",
       " ('05', 1914)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the query\n",
    "engine.execute(QUERY_STMT_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfa0ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the query to file\n",
    "query_to_file(QUERY_STMT_1, query_file_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
